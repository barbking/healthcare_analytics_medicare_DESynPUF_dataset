{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework Assignment for Graduate Course in Healthcare Analytics\n",
    "- Using DE-SynPUF files downloaded from https://www.cms.gov/Research-Statistics-Data-and-Systems/Downloadable-Public-Use-Files/SynPUFs/DE_Syn_PUF.html\n",
    "- Assignment: use methods related to naive bayes to predict depression as response/target\n",
    "- Using dataset already transformed from previous homeowrk assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and read file, use NaN for elements with no values\n",
    "from pandas import DataFrame, read_csv\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"combined_ben_sum_AddYear.csv\", na_values = ['no info', '.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset of data needed to answer assignment question #4\n",
    "newdf = df[['DESYNPUF_ID','Year','BENE_BIRTH_DT','BENE_SEX_IDENT_CD','BENE_RACE_CD',\n",
    "            'BENE_ESRD_IND','SP_STATE_CODE','BENE_COUNTY_CD','SP_ALZHDMTA',\n",
    "       'SP_CHF', 'SP_CHRNKIDN', 'SP_CNCR', 'SP_COPD','SP_DEPRESSN',\n",
    "       'SP_DIABETES', 'SP_ISCHMCHT', 'SP_OSTEOPRS', 'SP_RA_OA', 'SP_STRKETIA']]\n",
    "#print(newdf.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking to see how many duplicate rows, all columns except year\n",
    "# don't want to skew results if have same patient over 2008-2010 with same conditions repeated each year\n",
    "# only want to include if patient has developed new conditions\n",
    "dup_df = newdf[newdf.duplicated(['DESYNPUF_ID','BENE_BIRTH_DT','BENE_SEX_IDENT_CD','BENE_RACE_CD',\n",
    "            'BENE_ESRD_IND','SP_STATE_CODE','BENE_COUNTY_CD','SP_ALZHDMTA',\n",
    "       'SP_CHF', 'SP_CHRNKIDN', 'SP_CNCR', 'SP_COPD','SP_DEPRESSN',\n",
    "       'SP_DIABETES', 'SP_ISCHMCHT', 'SP_OSTEOPRS', 'SP_RA_OA', 'SP_STRKETIA']) == True].sort_values(by='DESYNPUF_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing duplicate rows, 1424410 removed\n",
    "df_no_dup_records_1 = newdf.drop_duplicates(['DESYNPUF_ID','BENE_BIRTH_DT','BENE_SEX_IDENT_CD','BENE_RACE_CD',\n",
    "            'BENE_ESRD_IND','SP_STATE_CODE','BENE_COUNTY_CD','SP_ALZHDMTA',\n",
    "       'SP_CHF', 'SP_CHRNKIDN', 'SP_CNCR', 'SP_COPD','SP_DEPRESSN',\n",
    "       'SP_DIABETES', 'SP_ISCHMCHT', 'SP_OSTEOPRS', 'SP_RA_OA', 'SP_STRKETIA'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also remove all recorcds after patient's first diagnosis, any future conditions post-depression diagnosis would not be a \n",
    "# predictor or cause of depression\n",
    "df_no_dup_records = df_no_dup_records_1.drop_duplicates(['DESYNPUF_ID','BENE_BIRTH_DT','SP_DEPRESSN'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate age (year column - year from BENE_BIRTH_DT)\n",
    "#newdf[\"Birth_Year\"] = pd.to_datetime(newdf['BENE_BIRTH_DT']).dt.year\n",
    "df_no_dup_records.loc[:,'Age'] = df_no_dup_records.loc[:,'Year'] - (pd.to_datetime(newdf.loc[:,'BENE_BIRTH_DT'],format='%Y%m%d').dt.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display and check age calculation working as expected\n",
    "display(df_no_dup_records.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each of the chronic condition col, prev data was 2 = no and 1 = yes\n",
    "# replace 2 with 0 so have 0 = no and 1 = yes\n",
    "for column in range(8,19):\n",
    "    df_no_dup_records.iloc[:,column] = df_no_dup_records.iloc[:,column].replace(2,0)\n",
    "# for BENE_ESRD_IND replace 'Y' with 1, 'N' is already set to 0\n",
    "df_no_dup_records.loc[:,'BENE_ESRD_IND'] = df_no_dup_records.loc[:,'BENE_ESRD_IND'].replace('Y',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_no_dup_records.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "- see HW8 for EDA on this dataset\n",
    "- classes unbalanced so using under-sampling method to balance\n",
    "- good summary here to learn more (https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "df_no_dup_records['SP_DEPRESSN'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class count\n",
    "count_class_0, count_class_1 = df_no_dup_records.SP_DEPRESSN.value_counts()\n",
    "\n",
    "# Divide by class\n",
    "df_class_0 = df_no_dup_records[df_no_dup_records['SP_DEPRESSN'] == 0]\n",
    "df_class_1 = df_no_dup_records[df_no_dup_records['SP_DEPRESSN'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class_0_under = df_class_0.sample(count_class_1)\n",
    "df_test_under = pd.concat([df_class_0_under, df_class_1], axis=0)\n",
    "\n",
    "print('Random under-sampling:')\n",
    "print(df_test_under.SP_DEPRESSN.value_counts())\n",
    "\n",
    "df_test_under.SP_DEPRESSN.value_counts().plot(kind='bar', title='Count (SP_DEPRESSN)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data\n",
    "X = df_test_under.iloc[:,[3,4,5,8,9,10,11,12,14,15,16,17,18,19]].values\n",
    "y = df_test_under.iloc[:,13].values\n",
    "\n",
    "#Splitting the data into Training Set and Test Set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test= train_test_split(X,y,test_size=0.3,random_state=0)\n",
    "\n",
    "#Normalizing the features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X= StandardScaler()\n",
    "X_train= sc_X.fit_transform(X_train)\n",
    "X_test= sc_X.transform(X_test)\n",
    "#print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting Classifier to Training Set. Create a classifier object here and call it classifierObj\n",
    "#Gaussian is parameter-less\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifierObj= GaussianNB()\n",
    "classifierObj.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
